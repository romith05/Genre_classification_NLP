{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "npaEOYk424F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLMOQFt91m9B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('DF_3Genres_Lyrics_En.csv')"
      ],
      "metadata": {
        "id": "ixEKqHiI1tth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "_b_waM3O1wgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "QiZJlqiF1x8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "DAAo-tst105W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "X5Sryby013pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the genre columns into a single 'genre' column\n",
        "def get_genre(row):\n",
        "    if row['Hip Hop'] == 1:\n",
        "        return 'hip hop'\n",
        "    elif row['Pop'] == 1:\n",
        "        return 'pop'\n",
        "    elif row['Rock'] == 1:\n",
        "        return 'rock'\n",
        "    else:\n",
        "        return 'unknown' # Handle cases where no genre is marked (should not happen with this dataset)\n",
        "\n",
        "df['genre'] = df.apply(get_genre, axis=1)\n",
        "\n",
        "# Drop the original genre columns\n",
        "df.drop(['Hip Hop', 'Pop', 'Rock'], axis=1, inplace=True)\n",
        "\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "5Jt-X1W_14GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporatory Data Analysis"
      ],
      "metadata": {
        "id": "NZN-QYJM3Tpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "genre_counts = df['genre'].value_counts().reset_index()\n",
        "genre_counts.columns = ['genre', 'count']\n",
        "\n",
        "fig = px.pie(genre_counts, values='count', names='genre', title='Distribution of Music Genres')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "pvIeKzSG17Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "all_lyrics = \" \".join(df['Lyric'])\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_lyrics)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XhMYQXUE18dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(df['genre'])"
      ],
      "metadata": {
        "id": "pVWH9f2d1848"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial of Combinations"
      ],
      "metadata": {
        "id": "aarSHGEf3-5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from joblib import dump\n",
        "\n",
        "# Try optional deps\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAVE_XGB = True\n",
        "except Exception:\n",
        "    HAVE_XGB = False\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    HAVE_SBERT = True\n",
        "except Exception:\n",
        "    HAVE_SBERT = False\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "DATA_PATH = \"songs.csv\"         # <- change to your file\n",
        "TEXT_COL  = \"Lyric\"\n",
        "TARGET_COL= \"genre\"\n",
        "\n",
        "SBERT_MODEL_NAME = \"all-mpnet-base-v2\"  # change if you want\n",
        "CV_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "N_JOBS = -1\n",
        "\n",
        "SAVE_DIR = \"artifacts\"\n",
        "BEST_MODEL_PATH = os.path.join(SAVE_DIR, \"best_pipeline.joblib\")\n",
        "LABEL_ENCODER_PATH = os.path.join(SAVE_DIR, \"label_encoder.joblib\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# UTIL: SBERT vectorizer as sklearn transformer\n",
        "# =========================\n",
        "class SBERTVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model_name=SBERT_MODEL_NAME, batch_size=64, normalize=True):\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "        self.normalize = normalize\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if not HAVE_SBERT:\n",
        "            raise ImportError(\"sentence-transformers not installed. pip install sentence-transformers\")\n",
        "        if self.model is None:\n",
        "            self.model = SentenceTransformer(self.model_name)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        embeddings = self.model.encode(\n",
        "            list(X),\n",
        "            batch_size=self.batch_size,\n",
        "            show_progress_bar=False,\n",
        "            normalize_embeddings=self.normalize\n",
        "        )\n",
        "        return np.asarray(embeddings, dtype=np.float32)\n",
        "\n",
        "# =========================\n",
        "# DATA\n",
        "# =========================\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.dropna(subset=[TEXT_COL, TARGET_COL])\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "X_text = df_train[TEXT_COL].astype(str).values\n",
        "y_str  = df_train[TARGET_COL].astype(str).values\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_str)\n",
        "\n",
        "# Save label encoder now (classes are final)\n",
        "dump(le, LABEL_ENCODER_PATH)\n",
        "\n",
        "# =========================\n",
        "# SCORERS & CV\n",
        "# =========================\n",
        "scorers = {\n",
        "    \"f1_macro\": make_scorer(f1_score, average=\"macro\"),\n",
        "    \"accuracy\": make_scorer(accuracy_score)\n",
        "}\n",
        "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# =========================\n",
        "# VECTORIZERS\n",
        "# =========================\n",
        "def tfidf_words():\n",
        "    return TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.9)\n",
        "\n",
        "def tfidf_chars():\n",
        "    # character n-grams shine on noisy lyrics\n",
        "    return TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2, lowercase=True)\n",
        "\n",
        "def count_words():\n",
        "    return CountVectorizer(ngram_range=(1,2), min_df=2)\n",
        "\n",
        "def sbert_vec():\n",
        "    return SBERTVectorizer(model_name=SBERT_MODEL_NAME)\n",
        "\n",
        "# =========================\n",
        "# CLASSIFIERS (with small, sane grids)\n",
        "# =========================\n",
        "def linear_svc(C=1.0):\n",
        "    return LinearSVC(C=C)\n",
        "\n",
        "def logreg(C=2.0):\n",
        "    # saga handles L1/L2; we keep default penalty='l2'\n",
        "    return LogisticRegression(C=C, max_iter=2000, solver=\"saga\", n_jobs=N_JOBS)\n",
        "\n",
        "def sgd(loss=\"hinge\", alpha=1e-4):\n",
        "    return SGDClassifier(loss=loss, alpha=alpha, random_state=RANDOM_STATE)\n",
        "\n",
        "def ridge(alpha=1.0):\n",
        "    return RidgeClassifier(alpha=alpha, random_state=RANDOM_STATE)\n",
        "\n",
        "def pa(C=1.0):\n",
        "    return PassiveAggressiveClassifier(C=C, random_state=RANDOM_STATE)\n",
        "\n",
        "def cnb():\n",
        "    return ComplementNB()\n",
        "\n",
        "def rbf_svm(C=2.0, gamma=\"scale\"):\n",
        "    return SVC(kernel=\"rbf\", C=C, gamma=gamma)\n",
        "\n",
        "def knn(k=7):\n",
        "    return KNeighborsClassifier(n_neighbors=k, metric=\"cosine\")\n",
        "\n",
        "def xgb_small():\n",
        "    if not HAVE_XGB:\n",
        "        return None\n",
        "    return XGBClassifier(\n",
        "        n_estimators=400, max_depth=6, learning_rate=0.05,\n",
        "        subsample=0.9, colsample_bytree=0.8,\n",
        "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
        "        random_state=RANDOM_STATE, n_jobs=N_JOBS\n",
        "    )\n",
        "\n",
        "# =========================\n",
        "# COMBOS\n",
        "# Each entry: (name, pipeline_factory)\n",
        "# The factory returns a ready sklearn Pipeline\n",
        "# =========================\n",
        "def make_sparse_pipeline(vectorizer, clf):\n",
        "    return Pipeline([\n",
        "        (\"vec\", vectorizer),\n",
        "        (\"clf\", clf)\n",
        "    ])\n",
        "\n",
        "def make_sparse_svd_tree(vectorizer, clf):\n",
        "    # Trees don't love high-dim sparse; reduce with SVD\n",
        "    return Pipeline([\n",
        "        (\"vec\", vectorizer),\n",
        "        (\"svd\", TruncatedSVD(n_components=300, random_state=RANDOM_STATE)),\n",
        "        (\"clf\", clf)\n",
        "    ])\n",
        "\n",
        "def make_dense_pipeline(vectorizer, clf, scale=False):\n",
        "    steps = [(\"vec\", vectorizer)]\n",
        "    if scale:\n",
        "        steps.append((\"scaler\", StandardScaler()))\n",
        "    steps.append((\"clf\", clf))\n",
        "    return Pipeline(steps)\n",
        "\n",
        "combos: List[Tuple[str, Pipeline]] = []\n",
        "\n",
        "# --- Sparse text: TF-IDF words ---\n",
        "combos += [\n",
        "    (\"tfidf_words + LinearSVC\",      make_sparse_pipeline(tfidf_words(), linear_svc(C=1.0))),\n",
        "    (\"tfidf_words + LogisticReg\",    make_sparse_pipeline(tfidf_words(), logreg(C=2.0))),\n",
        "    (\"tfidf_words + SGD(hinge)\",     make_sparse_pipeline(tfidf_words(), sgd(loss=\"hinge\", alpha=1e-4))),\n",
        "    (\"tfidf_words + SGD(log)\",       make_sparse_pipeline(tfidf_words(), sgd(loss=\"log_loss\", alpha=1e-4))),\n",
        "    (\"tfidf_words + Ridge\",          make_sparse_pipeline(tfidf_words(), ridge(alpha=1.0))),\n",
        "    (\"tfidf_words + PassiveAggressive\", make_sparse_pipeline(tfidf_words(), pa(C=1.0))),\n",
        "    (\"tfidf_words + ComplementNB\",   make_sparse_pipeline(tfidf_words(), cnb())),\n",
        "    (\"tfidf_words + NearestCentroid\",make_sparse_pipeline(tfidf_words(), NearestCentroid(metric=\"cosine\")))\n",
        "]\n",
        "\n",
        "# --- Sparse text: TF-IDF chars ---\n",
        "combos += [\n",
        "    (\"tfidf_chars + LinearSVC\",      make_sparse_pipeline(tfidf_chars(), linear_svc(C=1.0))),\n",
        "    (\"tfidf_chars + LogisticReg\",    make_sparse_pipeline(tfidf_chars(), logreg(C=2.0))),\n",
        "    (\"tfidf_chars + Ridge\",          make_sparse_pipeline(tfidf_chars(), ridge(alpha=1.0))),\n",
        "]\n",
        "\n",
        "# --- Sparse text: Count (for NB) ---\n",
        "combos += [\n",
        "    (\"count_words + ComplementNB\",   make_sparse_pipeline(count_words(), cnb())),\n",
        "]\n",
        "\n",
        "# --- Sparse -> Trees via SVD (optional XGB if available) ---\n",
        "if HAVE_XGB:\n",
        "    combos += [\n",
        "        (\"tfidf_words + SVD + XGB\",  make_sparse_svd_tree(tfidf_words(), xgb_small())),\n",
        "        (\"tfidf_chars + SVD + XGB\",  make_sparse_svd_tree(tfidf_chars(), xgb_small())),\n",
        "    ]\n",
        "\n",
        "# --- Dense embeddings: SBERT (if available) ---\n",
        "if HAVE_SBERT:\n",
        "    combos += [\n",
        "        (\"SBERT + LogisticReg\",      make_dense_pipeline(sbert_vec(), logreg(C=2.0), scale=True)),\n",
        "        (\"SBERT + RBF SVM\",          make_dense_pipeline(sbert_vec(), rbf_svm(C=2.0), scale=True)),\n",
        "        (\"SBERT + kNN(7)\",           make_dense_pipeline(sbert_vec(), knn(k=7), scale=False)),\n",
        "    ]\n",
        "    if HAVE_XGB:\n",
        "        combos += [(\"SBERT + XGB\",   make_dense_pipeline(sbert_vec(), xgb_small(), scale=False))]\n",
        "\n",
        "# =========================\n",
        "# RUN ALL COMBOS (cross-validate)\n",
        "# =========================\n",
        "results: List[Dict[str, Any]] = []\n",
        "\n",
        "print(f\"Running {len(combos)} combos with {CV_FOLDS}-fold Stratified CV...\")\n",
        "for name, pipe in combos:\n",
        "    print(f\"\\n‚ñ∂ {name}\")\n",
        "    try:\n",
        "        cvres = cross_validate(\n",
        "            pipe, X_text, y,\n",
        "            scoring=scorers, cv=cv, n_jobs=N_JOBS, return_train_score=False\n",
        "        )\n",
        "        res = {\n",
        "            \"name\": name,\n",
        "            \"f1_macro_mean\": float(np.mean(cvres[\"test_f1_macro\"])),\n",
        "            \"f1_macro_std\":  float(np.std(cvres[\"test_f1_macro\"])),\n",
        "            \"acc_mean\":      float(np.mean(cvres[\"test_accuracy\"])),\n",
        "            \"acc_std\":       float(np.std(cvres[\"test_accuracy\"]))\n",
        "        }\n",
        "        print(f\"   F1(macro): {res['f1_macro_mean']:.4f} ¬± {res['f1_macro_std']:.4f} | \"\n",
        "              f\"Acc: {res['acc_mean']:.4f} ¬± {res['acc_std']:.4f}\")\n",
        "        results.append(res)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Skipped due to error: {e}\")\n",
        "\n",
        "# =========================\n",
        "# PICK BEST & REFIT ON FULL DATA\n",
        "# =========================\n",
        "if not results:\n",
        "    raise RuntimeError(\"No successful runs. Check dependencies and data.\")\n",
        "\n",
        "res_df = pd.DataFrame(results).sort_values(by=[\"f1_macro_mean\", \"acc_mean\"], ascending=False)\n",
        "print(\"\\n===== Leaderboard (top 10) =====\")\n",
        "print(res_df.head(10).to_string(index=False))\n",
        "\n",
        "best_name = res_df.iloc[0][\"name\"]\n",
        "print(f\"\\nüèÜ Best combo: {best_name}\")\n",
        "\n",
        "best_pipe = None\n",
        "for name, pipe in combos:\n",
        "    if name == best_name:\n",
        "        best_pipe = pipe\n",
        "        break\n",
        "\n",
        "print(\"Fitting best pipeline on full dataset...\")\n",
        "best_pipe.fit(X_text, y)\n",
        "\n",
        "dump(best_pipe, BEST_MODEL_PATH)\n",
        "print(f\"Saved best pipeline to: {BEST_MODEL_PATH}\")\n",
        "print(f\"Saved label encoder to: {LABEL_ENCODER_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-A3fclDB2Uvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "res_df = pd.DataFrame(results).sort_values(by=[\"f1_macro_mean\", \"acc_mean\"], ascending=False)\n",
        "print(\"\\n===== Leaderboard (top 10) =====\")\n",
        "print(res_df.head(10).to_string(index=False))\n",
        "\n",
        "best_name = res_df.iloc[0][\"name\"]\n",
        "print(f\"\\nüèÜ Best combo: {best_name}\")\n",
        "\n",
        "best_pipe = None\n",
        "for name, pipe in combos:\n",
        "    if name == best_name:\n",
        "        best_pipe = pipe\n",
        "        break\n",
        "\n",
        "print(\"Fitting best pipeline on full dataset...\")\n",
        "best_pipe.fit(X_text, y)\n",
        "\n",
        "dump(best_pipe, BEST_MODEL_PATH)\n",
        "print(f\"Saved best pipeline to: {BEST_MODEL_PATH}\")\n",
        "print(f\"Saved label encoder to: {LABEL_ENCODER_PATH}\")"
      ],
      "metadata": {
        "id": "HkwNdLHr2Zg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the model performances\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot F1-macro scores\n",
        "ax[0].barh(res_df['name'], res_df['f1_macro_mean'], xerr=res_df['f1_macro_std'], capsize=5)\n",
        "ax[0].set_xlabel('Macro F1-score')\n",
        "ax[0].set_title('Model Performance (Macro F1-score)')\n",
        "ax[0].invert_yaxis() # To show the best performing model at the top\n",
        "\n",
        "# Plot Accuracy scores\n",
        "ax[1].barh(res_df['name'], res_df['acc_mean'], xerr=res_df['acc_std'], capsize=5, color='orange')\n",
        "ax[1].set_xlabel('Accuracy')\n",
        "ax[1].set_title('Model Performance (Accuracy)')\n",
        "ax[1].invert_yaxis() # To show the best performing model at the top\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MeNSZrJ52cwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Tuning Using GridSearch"
      ],
      "metadata": {
        "id": "tj4mVGcP4Kz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from joblib import load\n",
        "\n",
        "pipe = load(\"artifacts/best_pipeline.joblib\")\n",
        "\n",
        "param_grid = {\n",
        "    \"vec__ngram_range\": [(3,5), (4,6)],\n",
        "    \"clf__C\": [0.5, 1, 2, 5],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid, cv=5, scoring=\"f1_macro\", n_jobs=-1)\n",
        "grid.fit(X_text, y)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best CV F1 Macro:\", grid.best_score_)"
      ],
      "metadata": {
        "id": "BYtXQ8wY2pNI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}